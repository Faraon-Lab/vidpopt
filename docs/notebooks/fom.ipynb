{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures of Merit in `vipdopt`\n",
    "\n",
    "This notebook serves as a guide to using the figure of merit (FoM) class `FoM`. This\n",
    "class has a number of tools designed to abstract the notion of a FoM.\n",
    "\n",
    "The FoMs in `vipdopt` are based on the adjoint state method.<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1)\n",
    "\n",
    "For later cells to work, please run the following block first.\n",
    "\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) Lalau-Keraly, C. M., Bhargava, S., Miller, O. D. & Yablonovitch, E. Adjoint shape optimization applied to electromagnetic design. Opt. Express 21, 21693 (2013)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "import sys  \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=100)\n",
    "\n",
    "# Get vipdopt directory path from Notebook\n",
    "parent_dir = str(Path().resolve().parents[1])\n",
    "\n",
    "# Add to sys.path\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Imports from vipdopt\n",
    "from vipdopt.optimization import FoM, BayerFilterFoM, UniformMAEFoM\n",
    "from vipdopt.simulation import Power, Profile, LumericalSimulation, GaussianSource, DipoleSource, LumericalFDTD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `FoM` Class \n",
    "\n",
    "The `FoM` class at its core is defined by two functions:\n",
    "* A function that computes the FoM (`fom_func`)\n",
    "* A function that computes the gradient of the FoM for optimization (`grad_func`)\n",
    "\n",
    "To instantiate a `FoM` object, these two functions will need to be provided as well as\n",
    "a number of other arguments:\n",
    "* Which polarization to use out of TE, TM, or TE + TM (`polarization`)\n",
    "* A list of sources to enable in the forward / adjoint simulations (`fwd_srcs` / `adj_srcs`)\n",
    "* A list of monitors in the forward / adjoint simulations from which data will be accessed in computing the FoM (`fwd_monitors` / `adj_monitors`)\n",
    "* A list of the indices of frequencies being minimized / maximized in the optimization (`pos_max_freqs` / `neg_min_freqs`)\n",
    "\n",
    "Note that monitors will output data of shape N x ... x L where L is the total number of\n",
    "frequencies. So specifying that `pos_max_freqs=range(L)` is equivalent to saying that\n",
    "we're maximizing over every frequency.\n",
    "\n",
    "In the following example, we make use of the `BayerFilterFoM` subclass, which has\n",
    "pre-defined functions for the FoM and gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: all object names are based on those in \"simulation_example.json\"\n",
    "\n",
    "\n",
    "# List of sources needed to make forward / adjoint simulations\n",
    "forward_sources = [\n",
    "    GaussianSource('forward_src_x'),\n",
    "]\n",
    "adjoint_sources = [\n",
    "    GaussianSource('forward_src_x'),\n",
    "    DipoleSource('adjoint_src_0x'),\n",
    "]\n",
    "\n",
    "# Monitors whose data is necessary for computing the FoM\n",
    "forward_monitors = [\n",
    "    Power('focal_monitor_0'),\n",
    "    Power('transmission_monitor_0'),\n",
    "    Profile('design_efield_monitor'),\n",
    "]\n",
    "adjoint_monitors = [\n",
    "    Profile('design_efield_monitor'),\n",
    "]\n",
    "\n",
    "fom = BayerFilterFoM(\n",
    "    polarization='TE',\n",
    "    fwd_srcs=forward_sources,\n",
    "    adj_srcs=adjoint_sources,\n",
    "    fwd_monitors=forward_monitors,\n",
    "    adj_monitors=adjoint_monitors,\n",
    "    pos_max_freqs=list(range(60)),  # We're maximizing all 60 frequencies \n",
    "    neg_min_freqs=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each FoM is also able to create its forward and adjoint simulations given a provided\n",
    "\"base simulation\" to start from. It achieves this by disabling all sources except for\n",
    "those specified, and by replacing its `Monitor`'s with their counterparts in the \n",
    "simulation.\n",
    "\n",
    "So long as the FoM's monitors and sources share a name with a corresponding object in \n",
    "the base simulation, the `create_fwd_sim` and `create_adj_sim` methods will create \n",
    "their respective simulation variants automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_sim = LumericalSimulation('simulation_example.json')\n",
    "\n",
    "fwd_sim = fom.create_forward_sim(base_sim)[0]  # Access first element, as method returns a list\n",
    "adj_sim = fom.create_adjoint_sim(base_sim)[0]\n",
    "\n",
    "# Run the simulations and save the monitor data\n",
    "fdtd = LumericalFDTD()\n",
    "\n",
    "fdtd.save('fwd_sim.fsp', fwd_sim)\n",
    "fdtd.save('adj_sim.fsp', adj_sim)\n",
    "\n",
    "fdtd.addjob('fwd_sim.fsp')\n",
    "fdtd.addjob('adj_sim.fsp')\n",
    "\n",
    "fdtd.runjobs()\n",
    "fdtd.reformat_monitor_data([fwd_sim, adj_sim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the FoM has been created, and its forward and adjoint simulations have been \n",
    "run, we can finally compute its functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = fom.compute_fom()\n",
    "\n",
    "g = fom.compute_grad()\n",
    "\n",
    "print(f'The FoM is {f}')\n",
    "print(f'The gradient is {g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetic Functions on `FoM`'s\n",
    "\n",
    "Every `FoM` is equipped with arithmetic functions in order to combine separate figures.  The supported operations are as follows:\n",
    "* Addition / subtraction\n",
    "* Multiplication\n",
    "* Scalar multiplication / division\n",
    "\n",
    "Combining `FoM` in this way will result in a `SuperFoM` which represents a weighted sum of the different `FoM`'s. A `SuperFoM` contains two lists:\n",
    "* a list of tuples of `FoM`s\n",
    "* a list of weights to corresponding to each tuple\n",
    "\n",
    "Each tuple is a product of all of the FoMs it contains. So if you had the tuple\n",
    "`(f(x),)`, that represents just $f(x)$ by itself, whereas `(f(x), g(x))` would correspond to a $f(x)\\cdot g(x)$ factor. Using the supported operations can therefore produce combined FoMs of the form $\\text{FoM}(x) = w_1 f(x) + w_2 g(x) h(x)$ , where $f, g, h$ are FoMs, possibly `SuperFoM`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This FoM takes an array and measures the absolute error from a pre-determined\n",
    "# constant (i.e. 0.5) at each point. It doesn't require monitors, or even a\n",
    "# simulation to be run and is merely for examples / testing.\n",
    "fom1 = UniformMAEFoM('TE', [], [], [], [], [0], [0], constant=0.5)\n",
    "fom2 = UniformMAEFoM('TE', [], [], [], [], [0], [0], constant=1.5)\n",
    "\n",
    "combined_fom = fom1 + fom2\n",
    "\n",
    "test_array = np.arange(9).reshape((3, 3))\n",
    "\n",
    "# The combination should evaluate to the same value\n",
    "fom1_val = fom1.compute_fom(test_array)\n",
    "fom2_val = fom2.compute_fom(test_array)\n",
    "assert fom1_val + fom2_val == combined_fom.compute_fom(test_array)\n",
    "\n",
    "# We can also change the weights of the FoMs\n",
    "combined_fom = 1.5 * fom1 + 2 * fom2\n",
    "assert combined_fom.compute_fom(test_array) == 1.5 * fom1_val + 2 * fom2_val\n",
    "\n",
    "# Multiplication works as expected\n",
    "combined_fom = fom1 * fom2\n",
    "assert combined_fom.compute_fom(test_array) == fom1_val * fom2_val\n",
    "\n",
    "# Computing the gradient also works as expected (applying the product rule)\n",
    "fom1_grad = fom1.compute_grad(test_array)\n",
    "fom2_grad = fom2.compute_grad(test_array)\n",
    "fom1_arr = fom1.compute_fom(test_array, sum_values=False)  # When used in computing the gradient, the FoM is not summed\n",
    "fom2_arr = fom2.compute_fom(test_array, sum_values=False)\n",
    "np.testing.assert_equal(combined_fom.compute_grad(test_array), fom1_grad * fom2_arr + fom1_arr * fom2_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Weighting and Performance Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a New Figure of Merit\n",
    "\n",
    "The `FoM` class at its core is defined by two functions:\n",
    "* A function that computes the FoM (`fom_func`)\n",
    "* A function that computes the gradient of the FoM for optimization (`grad_func`)\n",
    "\n",
    "That's it! In fact, to implement your own FoM class, all you need to do is provide these\n",
    "two functions and it will be compatible with all other optimization code.\n",
    "\n",
    "In the following examples, we use the following FoM:\n",
    "$$FoM(x) = x^2 \\qquad FoM'(x) = 2 x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fom_func(x):\n",
    "    return x ** 2\n",
    "\n",
    "def gradient_func(x):\n",
    "    return 2 * x\n",
    "\n",
    "# Here we create our FoM using the FoM class directly\n",
    "fom = FoM('TE', [], [], [], [], fom_func, gradient_func, [0], [])\n",
    "assert fom.compute_fom(5) == 25\n",
    "assert fom.compute_grad(5) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can instead define our own FoM subclass\n",
    "class SquareFoM(FoM):\n",
    "\n",
    "    # We don't need the majority of the arguments for a normal FoM since we dont need a simulation\n",
    "    def __init__(\n",
    "            self,\n",
    "            pos_max_freqs,\n",
    "            neg_min_freqs,\n",
    "            spectral_weights=np.array(1),\n",
    "    ) -> None:\n",
    "        # For all of the unnecessary arguments we use arbitrary values\n",
    "        super().__init__(\n",
    "            'TE',\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            self._square_fom,\n",
    "            self._square_gradient,\n",
    "            pos_max_freqs,\n",
    "            neg_min_freqs,\n",
    "            spectral_weights,\n",
    "        )\n",
    "    \n",
    "    def _square_fom(self, x):\n",
    "        return x ** 2\n",
    "    \n",
    "    def _square_gradient(self, x):\n",
    "        return 2 * x\n",
    "\n",
    "fom = SquareFoM([1], [])\n",
    "assert fom.compute_fom(5) == 25\n",
    "assert fom.compute_grad(5) == 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
